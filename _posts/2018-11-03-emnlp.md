---
layout: post
title: EMNLP 2018
---

Once a year, I try to attend an NLP or ML conference, usually picking the one that is closest in terms of location.
In 2016 I was at [ACL in Berlin](https://chauff.github.io/2016-08-12-acl2016/) :de:, in 2017 I attended 
[ICLR in Toulon](https://chauff.github.io/2017-04-25-iclr2017/) :fr: and this year I made the trip to [EMNLP in Brussels](http://emnlp2018.org/) :belgium:.

EMNLP is short for *Conference on Empirical Methods in Natural Language Processing* and arguably, this conference
is most closely related to my own research area IR. As almost all reputable ML/NLP venues, the conference had a record number of
submissions (>2100) and participants (~2500) this year. Overall, 549 papers were accepted and split between oral and poster sessions.
Everything happened in parallel (not just oral sessions, but also poster sessions), apart from they keynotes and 
best paper session, which means i missed about 90% of everything that was out there.

Before the 3-day conference itself, there were two days of workshops and co-located conferences. I attended the 
[Search-Oriented Conversational AI workshop](https://scai.info/) and the [BlackboxNLP workshop](https://blackboxnlp.github.io/).
Each one had several hundred participants, with many great talks! 

I was impressed that, despite the focus on the empirical work (or maybe because of it?), many presentations were void
of almost any formula - which is quite a contrast to SIGIR or CIKM where it sometimes seems like a challenge to put in
as many formulas as possible.

What I also took away from the conference is that, often, *IR baselines* (which seems to be the way of referring to anything IR-ish) are used as poorly performing baselines whose effectiveness is well below even the worst deep models. I have only seen a few presentations and certainly have not looked at more than a small share of the papers, but this slide sums up pretty well what I perceive as the majority opinion in the NLP community:

<img src="../img/emnlp-ir.png" width="550px">

EMNLP does have an IR/text mining track, nothing stops us from pushing back on this narrative :muscle:!

The EMNLP proceedings can be found [here](https://aclanthology.coli.uni-saarland.de/events/emnlp-2018). 

Here are the works that stood out to me, these tended to be either papers about conversations and dialogues, papers with a strong IR component or dataset papers:

1. [Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering](https://aclanthology.coli.uni-saarland.de/papers/D18-1053/d18-1053). Question answering is typically a two-step process: (1) a retrieval round to get an initial list of documents and (2) a reading comprehension approach to zoom in on the answer to the input question. *"However, since  traditional information retrieval systems are not effective in obtaining documents with a high probabil-
ity of containing answers, they lower the performance of QA systems." The authors' solution: rank paragraphs instead. One of the datasets used in this paper is **CuratedTREC**, which I have heard a few times at EMNLP, still not sure what exactly that refers to. But at least one TREC dataset is popular in the NLP community!

29. [Deep Relevance Ranking Using Enhanced Document-Query Interactions](https://aclanthology.coli.uni-saarland.de/papers/D18-1211/d18-1211). This work proposes several extensions of the *Deep Relevance Matching Model* (DRMM), which outperform another recent deep learning IR model (PACRR). The paper is nicely argued for and the model looks good (PACRR is to my knowledge one of the best deep IR models), however, the model's effectiveness on the TREC Robust 2004 corpus is not great: the best MAP reported is 0.27 (with BM25 trailing at 0.24 and PACRR at 0.26) as the original TREC run submissions in 2004 [reached 0.33](https://trec.nist.gov/pubs/trec13/papers/ROBUST.OVERVIEW.pdf). This is a general issue with deep models in IR (that I am also guilty of), we tend not to look beyond BM25 baselines. Especially the initial TREC run results are still very tough to beat today, even with all the deep goodness!

1. :trophy: [How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks](https://aclanthology.coli.uni-saarland.de/papers/D18-1546/d18-1546). A award-winning short paper that takes a very critical look at reading comprehension models and datasets. In its own words: *"Many recent papers address reading comprehension,  where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. [...] we establish sensible baselines for the bAbI, SQuAD, CTB, CNN and Who-did-What datasets, finding that question- and passage-ONLY models often perform surprisingly well."*

2. :trophy: [MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling](https://aclanthology.coli.uni-saarland.de/papers/D18-1547/d18-1547). The NLP community values dataset papers. This one describes how a *"fully-labelled collection of [10k] human-human written conversations spanning over multiple domains and topics"* has been collected and annotated. An impressive amount of work required to push the boundaries of data-driven conversational agents.

3. [A strong baseline for question relevancy ranking](https://aclanthology.coli.uni-saarland.de/papers/D18-1515/d18-1515). Having complained about weak IR baselines, I had to include this paper. It actually uses Google's search engine ranking as *IR baseline* and manages to beat it on two SemEval community question answering tasks!

4. [Pyramidal Recurrent Unit for Language Modeling](https://aclanthology.coli.uni-saarland.de/papers/D18-1491/d18-1491)

5. [Word Mover's Embedding: From Word2Vec to Document Embedding](https://aclanthology.coli.uni-saarland.de/papers/D18-1482/d18-1482)

6. [NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval](https://aclanthology.coli.uni-saarland.de/papers/D18-1478/d18-1478)

7. [Chargrid: Towards Understanding 2D Documents](https://aclanthology.coli.uni-saarland.de/papers/D18-1476/d18-1476)

8. [Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks](https://aclanthology.coli.uni-saarland.de/papers/D18-1472/d18-1472)

9. [Speed Reading: Learning to Read ForBackward via Shuttle](https://aclanthology.coli.uni-saarland.de/papers/D18-1474/d18-1474)

10. [NEXUS Network: Connecting the Preceding and the Following in Dialogue Generation](https://aclanthology.coli.uni-saarland.de/papers/D18-1463/d18-1463)

11. [A Neural Local Coherence Model for Text Quality Assessment](https://aclanthology.coli.uni-saarland.de/papers/D18-1464/d18-1464)

12. [What Makes Reading Comprehension Questions Easier?](https://aclanthology.coli.uni-saarland.de/papers/D18-1453/d18-1453)

13. [Commonsense for Generative Multi-Hop Question Answering Tasks](https://aclanthology.coli.uni-saarland.de/papers/D18-1454/d18-1454)

14. [Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text](https://aclanthology.coli.uni-saarland.de/papers/D18-1455/d18-1455)

15. [Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints](https://aclanthology.coli.uni-saarland.de/papers/D18-1431/d18-1431)

16. [Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task](https://aclanthology.coli.uni-saarland.de/papers/D18-1425/d18-1425)

17. [AirDialogue: An Environment for Goal-Oriented Dialogue Research](https://aclanthology.coli.uni-saarland.de/papers/D18-1419/d18-1419)

18. [Learning Neural Templates for Text Generation](https://aclanthology.coli.uni-saarland.de/papers/D18-1356/d18-1356)

19. [Bayesian Compression for Natural Language Processing](https://aclanthology.coli.uni-saarland.de/papers/D18-1319/d18-1319)

20. [Generating Natural Language Adversarial Examples](https://aclanthology.coli.uni-saarland.de/papers/D18-1316/d18-1316)

21. [Evaluating the Utility of Hand-crafted Features in Sequence Labelling](https://aclanthology.coli.uni-saarland.de/papers/D18-1310/d18-1310)

22. [WikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community](https://aclanthology.coli.uni-saarland.de/papers/D18-1305/d18-1305)

23. [Training Millions of Personalized Dialogue Agents](https://aclanthology.coli.uni-saarland.de/papers/D18-1298/d18-1298)

24. [Towards Less Generic Responses in Neural Conversation Models: A Statistical Re-weighting Method](https://aclanthology.coli.uni-saarland.de/papers/D18-1297/d18-1297)

25. [HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering](https://aclanthology.coli.uni-saarland.de/papers/D18-1259/d18-1259)

26. [Possessors Change Over Time: A Case Study with Artworks](https://aclanthology.coli.uni-saarland.de/papers/D18-1251/d18-1251)

27. [QuAC: Question Answering in Context](https://aclanthology.coli.uni-saarland.de/papers/D18-1241/d18-1241)

28. [Interpretation of Natural Language Rules in Conversational Machine Reading](https://aclanthology.coli.uni-saarland.de/papers/D18-1233/d18-1233)

30. [What It Takes to Achieve 100 Percent Condition Accuracy on WikiSQL](https://aclanthology.coli.uni-saarland.de/papers/D18-1197/d18-1197)

31. [Mapping Language to Code in Programmatic Context](https://aclanthology.coli.uni-saarland.de/papers/D18-1192/d18-1192)

32. [A dataset and baselines for sequential open-domain question answering](https://aclanthology.coli.uni-saarland.de/papers/D18-1134/d18-1134)

33. [Do explanations make VQA models more predictable to a human?](https://aclanthology.coli.uni-saarland.de/papers/D18-1128/d18-1128)

34. [A Dataset for Telling the Stories of Social Media Videos](https://aclanthology.coli.uni-saarland.de/papers/D18-1117/d18-1117)

35. [A Dataset for Document Grounded Conversations](https://aclanthology.coli.uni-saarland.de/papers/D18-1076/d18-1076)

36. [Increasing In-Class Similarity by Retrofitting Embeddings with Demographic Information](https://aclanthology.coli.uni-saarland.de/papers/D18-1070/d18-1070)

39. [Privacy-preserving Neural Representations of Text](https://aclanthology.coli.uni-saarland.de/papers/D18-1001/d18-1001)





