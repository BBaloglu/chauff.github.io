---
layout: post
title: Interesting finds at ICLR 2019
---

Browsing through the accepted papers of ICLR 2019 a few works were particularly appealing to me, given my IR viewpoint. Besides that, the open review model also leads to some [public soul searching over decisions](https://openreview.net/forum?id=rJlWOj0qF7).

1. [No Training Required: Exploring Random Encoders for Sentence Classification](https://openreview.net/forum?id=BkgPajAcY7). The authors have a clear motivation for their work: *"it is unclear how much trained sentence-encoding architectures improve over the raw word embeddings, and what aspect of such architectures is responsible for any improvement"*. To gain a better understanding of the empirical performance of sentence embeddings the authors ask the following: *"given a set of word embeddings, how can we maximize classification accuracy on the transfer tasks WITHOUT any training?"* It turns out that random parametrizations go a long way once a fair manner of comparison is established. The authors include a few critical remarks in their article, including *"baselines need more love"* and *"... such in-full comparisons are simply not feasible, and often not appreciated by reviewers anyway"*.

2. [A comprehensive, application-oriented study of catastrophic forgetting in DNNs](https://openreview.net/forum?id=BkloRs0qK7)
3. [Generative Question Answering: Learning to Answer the Whole Question](https://openreview.net/forum?id=Bkx0RjA9tX)
4. [Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution](https://openreview.net/forum?id=ByME42AqK7)
5. [FlowQA: Grasping Flow in History for Conversational Machine Comprehension](https://openreview.net/forum?id=ByftGnR9KX)
6. [Adaptive Input Representations for Neural Language Modeling](https://openreview.net/forum?id=ByxZX20qFQ)
7. [CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model](https://openreview.net/forum?id=H1MgjoR9tQ)
8. [Understanding Composition of Word Embeddings via Tensor Decomposition](https://openreview.net/forum?id=H1eqjiCctX)
9. [Structured Neural Summarization](https://openreview.net/forum?id=H1ersoRqtm)
10. [code2seq: Generating Sequences from Structured Representations of Code](https://openreview.net/forum?id=H1gKYo09tX)
11. [Analysing Mathematical Reasoning Abilities of Neural Models](https://openreview.net/forum?id=H1gR5iR5FX)
12. [Benchmarking Neural Network Robustness to Common Corruptions and Perturbations](https://openreview.net/forum?id=HJz6tiCqYm)
13. [Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering](https://openreview.net/forum?id=HkfPSh05K7)
14. [Visualizing and Understanding Generative Adversarial Networks](https://openreview.net/forum?id=Hyg_X2C5FX)
15. [Stable Recurrent Models](https://openreview.net/forum?id=Hygxb2CqKm)
16. [Discovery of natural language concepts in individual units](https://openreview.net/forum?id=S1EERs09YQ)

17. [Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension](https://openreview.net/forum?id=S1lhbnRqF7) This paper tackles an interesting task: tracking the state changes of entities on a per-sentence level in procedural text. The [ProPara](http://data.allenai.org/propara/) dataset was the authors' main empirical target - it requires algoritms to identify actions and track the state changes of the involved entities. Here :point_down: is a concrete example from the ProPara website: 
<img src="http://data.allenai.org/propara/img/participant-grid-simple.JPG" width="600px">
The paper shows off how much engineering and reworking of existing models is required to pull off a good performance on this task. It is also a task that is far from being solved (i.e. we are still a long way off from the human performance levels).

18. [Representation Degeneration Problem in Training Natural Language Generation Models](https://openreview.net/forum?id=SkEYojRqtm) Here, the authors take an empirical observation that is undesired (namely that the training of natural language generation models leads to word embddings which are distributed in a narrow subspace), hypothesize about reasons for the behaviour and then "fix" the issue by a novel regularization approach. I like the mix of theoretical and empirical work in this one!

19. [Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering](https://openreview.net/forum?id=Syl7OsRqY7) is for me a prototypical ICLR paper. It contains experiments on two _very_ recent datasets ([Qangaroo WikiHop](https://qangaroo.cs.ucl.ac.uk/) and [TrivaQA](http://nlp.cs.washington.edu/triviaqa/)) that pose challenging question answering tasks which require reasoning over several evidence documents. The introduced model (which once more makes good use of the task knowledge) is state-of-the-art as of September 14, 2018 as seen in this result table :point_down::
<img src="../img/iclr2019-september14.png" width="600px">

20. [Wizard of Wikipedia: Knowledge-Powered Conversational Agents](https://openreview.net/forum?id=r1l73iRqKm) One of the main issues in the current quest for intelligent open-domain dialogue agents is the lack of high-quality and large-scale training data. This paper describes the collection of such a dataset as well as a number of high-quality end-to-end trained dialogue models. The authors have collected 22,311 conversations *grounded with Wikipedia knowledge* with on average 9 turns, on more than 1000 topics (here: a topic is a Wikipedia article). Each dialogue was created by two participants (crowd-workers) who engaged in chit-chat and had distinct roles: one played the role of a knowledgeable expert (the *wizard*) while the other was a learner. The wizard *has access to an information retrieval system that shows them paragraphs from Wikipedia possibly relevant to the conversation, which are unobserved by the apprentice. Before each conversation turn the wizard can read these paragraphs and then potentially base their next reply on that observed knowledge.* The presented models (retrieval-based and generative) replace the wizard in the conversation. The retrieval-based approaches (i.e. the retrieval of a valid response from the training data set) work outperform the generation approaches:
<img src="../img/iclr2019-wizardResults.png" width="600px">
Once more, IR is considered as pretty much useless, the IR baseline in the table :point_up: is just described as *a [..] baseline, which uses simple word overlap*. IR can definitely do better. 
Lastly, here :point_down: is an interesting conversation between a retrieval-based model and a human about [toga parties](https://en.wikipedia.org/wiki/Toga_party). Not much learning going on, but a lot of love for tag parties. There are probably many interesting conversations in the training dataset, available at http://www.parl.ai/.
<img src="../img/iclr2019-wizardConversation.png" width="600px">

21. [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://openreview.net/forum?id=rJ4km2R5t7) This paper introduces a benchmark that contains a suite of tasks for natural language understanding, comes with a [leaderboard](https://gluebenchmark.com/leaderboard) and a host of good-quality baselines. As leaderboards in the NLP context typically mean a hidden test set to avoid overfitting (which makes an error analysis and a fine-grained comparison between approaches that goes beyond *my number is higher than your number* impossible), the authors introduce an additional [diagnostic dataset](https://gluebenchmark.com/diagnostics). Every time I see a paper like this, I want to see a similar effort in IR. But sadly, IR conferences are not yet at a stage where dataset/benchmark papers come through easily. 

25. [Learning to Represent Edits](https://openreview.net/forum?id=BJl6AjC5F7) is an allrounder paper, presenting a task (*learn to represent the salient information of an edit and [use it] to apply edits to new inputs*), a public dataset and a number of neural baselines applied to natural language and source code edit data. The paper could have also easily fit into EMNLP or ICSE (the *International Conference on Software Engineering*) instead. What I really like here though is the writeup, and the way domain knowledge is cleverly encoded. A point to nag about is the IR baseline they employ: TF.IDF. There are better non-neural ways to compute the semantic similarity between short pieces of text. 

29. [Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks](https://openreview.net/forum?id=B1l6qiR5F7) Let me quote the meta-reviewer: *This paper presents a substantially new way of introducing a syntax-oriented inductive bias into sentence-level models for NLP without explicitly injecting linguistic knowledge. This is a major topic of research in representation learning for NLP, so to see something genuinely original work well is significant. All three reviewers were impressed by the breadth of the experiments and by the results, and this will clearly be among the more ambitious papers presented at this conference.* The authors revise the vanilla RNN model to incorporate syntax information in a clever way - based on a simple observation (*In general, natural language is governed by a tree structure:  smaller units (e.g., phrases) are nested within larger units (e.g., clauses).*) and a change of one of the RNN update functions. 

30. [Learning to Make Analogies by Contrasting Abstract Relational Structure](https://openreview.net/forum?id=SylLYsCcFm) This is great paper that takes a specific cognitive process (visual analogical reasoning) and explores to what extent neural nets are able to learn and reason about visual raw data. The figure below (from the paper) shows off what analogical reasoning entails and how the authors frame the problem. The key finding here is that not the neural architecture is of importance, but the clever selection of training data. Having such a concise problem and creating a corresponding dataset to investigate neural nets' ability to cope with the problem is something that is largely absent in IR. There are no small concise problems (or at least we don't consider IR tasks in this way) and so such precise diagnostics are hard to come by.
<img src="../img/iclr2019-analogicalReasoning.png" width="600px">

