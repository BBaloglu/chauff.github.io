---
layout: post
title: Interesting finds at ICLR 2019
---

Browsing through the accepted papers of ICLR 2019, a few works were particularly appealing to me - given my IR/applied NLP viewpoint:

1. [No Training Required: Exploring Random Encoders for Sentence Classification](https://openreview.net/forum?id=BkgPajAcY7). The authors have a clear motivation for their work: *"it is unclear how much trained sentence-encoding architectures improve over the raw word embeddings, and what aspect of such architectures is responsible for any improvement"*. To gain a better understanding of the empirical performance of sentence embeddings the authors ask the following: *"given a set of word embeddings, how can we maximize classification accuracy on the transfer tasks WITHOUT any training?"* It turns out that random parametrizations go a long way once a fair manner of comparison is established. The authors include a few critical remarks in their article, including *"baselines need more love"* and *"... such in-full comparisons are simply not feasible, and often not appreciated by reviewers anyway"*.

2. [A comprehensive, application-oriented study of catastrophic forgetting in DNNs](https://openreview.net/forum?id=BkloRs0qK7)
3. [Generative Question Answering: Learning to Answer the Whole Question](https://openreview.net/forum?id=Bkx0RjA9tX)
4. [Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution](https://openreview.net/forum?id=ByME42AqK7)
5. [FlowQA: Grasping Flow in History for Conversational Machine Comprehension](https://openreview.net/forum?id=ByftGnR9KX)
6. [Adaptive Input Representations for Neural Language Modeling](https://openreview.net/forum?id=ByxZX20qFQ)
7. [CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model](https://openreview.net/forum?id=H1MgjoR9tQ)
8. [Understanding Composition of Word Embeddings via Tensor Decomposition](https://openreview.net/forum?id=H1eqjiCctX)
9. [Structured Neural Summarization](https://openreview.net/forum?id=H1ersoRqtm)
10. [code2seq: Generating Sequences from Structured Representations of Code](https://openreview.net/forum?id=H1gKYo09tX)
11. [Analysing Mathematical Reasoning Abilities of Neural Models](https://openreview.net/forum?id=H1gR5iR5FX)
12. [Benchmarking Neural Network Robustness to Common Corruptions and Perturbations](https://openreview.net/forum?id=HJz6tiCqYm)
13. [Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering](https://openreview.net/forum?id=HkfPSh05K7)
14. [Visualizing and Understanding Generative Adversarial Networks](https://openreview.net/forum?id=Hyg_X2C5FX)
15. [Stable Recurrent Models](https://openreview.net/forum?id=Hygxb2CqKm)
16. [Discovery of natural language concepts in individual units](https://openreview.net/forum?id=S1EERs09YQ)
17. [Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension](https://openreview.net/forum?id=S1lhbnRqF7)
18. [Representation Degeneration Problem in Training Natural Language Generation Models](https://openreview.net/forum?id=SkEYojRqtm)
19. [Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering](https://openreview.net/forum?id=Syl7OsRqY7)
20. [Wizard of Wikipedia: Knowledge-Powered Conversational Agents](https://openreview.net/forum?id=r1l73iRqKm)
21. [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://openreview.net/forum?id=rJ4km2R5t7)
22. [BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop](https://openreview.net/forum?id=rJeXCo0cYX)
23. [Large-Scale Answerer in Questioner's Mind for Visual Dialog Question Generation](https://openreview.net/forum?id=rkgT3jRct7)

24. [Synthetic Datasets for Neural Program Synthesis](https://openreview.net/forum?id=ryeOSnAqYm)
25. [Learning to Represent Edits](https://openreview.net/forum?id=BJl6AjC5F7) is an allrounder paper, presenting a task (*learn to represent the salient information of an edit and [use it] to apply edits to new inputs*), a public dataset and a number of neural baselines applied to natural language and source code edit data. The paper does not look like a typical ICLR paper (it is very applied), it could have easily fit into EMNLP or ICSE (the *International Conference on Software Engineering*) instead. What I really like here though is the writeup, and the way domain knowledge is cleverly encoded. A point to nag about is the IR baseline they employ: TF.IDF. There are better non-neural wasy to compute the semantic similarity between short pieces of text. 

29. [Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks](https://openreview.net/forum?id=B1l6qiR5F7) Let me quote the meta-reviewer: *This paper presents a substantially new way of introducing a syntax-oriented inductive bias into sentence-level models for NLP without explicitly injecting linguistic knowledge. This is a major topic of research in representation learning for NLP, so to see something genuinely original work well is significant. All three reviewers were impressed by the breadth of the experiments and by the results, and this will clearly be among the more ambitious papers presented at this conference.* The authors revise the vanilla RNN model to incorporate syntax information in a clever way - based on a simple observation (*In general, natural language is governed by a tree structure:  smaller units (e.g., phrases) are nested within larger units (e.g., clauses).*) and a change of one of the RNN update functions. 

30. [Learning to Make Analogies by Contrasting Abstract Relational Structure](https://openreview.net/forum?id=SylLYsCcFm) This is great paper that takes a specific cognitive process (visual analogical reasoning) and explores to what extent neural nets are able to learn and reason about visual raw data. The figure below (from the paper) shows off what analogical reasoning entails and how the authors frame the problem. The key finding here is that not the neural architecture is of importance, but the clever selection of training data. Having such a concise problem and creating a corresponding dataset to investigate neural nets' ability to cope with the problem is something that is largely absent in IR. There are no small concise problems (or at least we don't consider IR tasks in this way) and so such precise diagnostics are hard to come by.
<img src="../img/iclr2019-analogicalReasoning.png" width="600px">

